[
  {
    "objectID": "Ethics_Blog.html",
    "href": "Ethics_Blog.html",
    "title": "An Analyisis of Data Ethics",
    "section": "",
    "text": "Considerations:\n\nWhat is the permission structure for using the data? Was it followed?\n\nFrom Wong’s Guardian Article, we learn that the permission structure claimed to be “academic research” for the users who consented to participate in the personality test. I compared the following information with my own research of Facebook’s permissions structure. According to Wong, Facebook’s privacy policy did allow researchers to collect not only direct user data, but also that of friends (Wong 2018). Facebook does confirm this, in both court hearings and through legal documentation I was able to find. However, because friends did not explicitly give permission, this permission structure was compromised. Confessore builds on this, adding that Facebook’s own policy required that apps/tests ask for “a clear privacy policy and user authorization” and that Facebook has the ability to revoke data access when misuse is detected, but FB did not sufficiently alert impacted users (Confessore, 2018). Thus, although a formal permission structure existed (Facebook’s platform rules, app disclosure), it was not followed in practice by the parties involved.\n\nWhat was the consent for recruiting participants?\n\nAccording to the Wong through the Guardian, only a few thousand users who ended up installing the personality test app knowingly supplied their data under the pretense of “academic research”. None of their FB friends were asked directly for consent. From Confessore’s article, we get the picture that Facebook’s terms required “informed consent” for data usage, yet the app’s true purpose (political targeting) was not disclosed (Confessore, 2018). Thus, the consent given was incomplete: it was partially informed and did not cover the large network of friends whose data was used.\n\nIs the data being used in unintended ways?\n\nBoth articles make it clear that the data was being used in dishonest and manipulative ways. Wong details how Cambridge Analytica created psychographic models by transforming Facebook “likes” and friend networks into political persuasion mechanisms that could be weaponized by campaign strategists (Wong, 2018). Researchers inferred user traits using the OCEAN model (Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism), which allowed them to predict how individuals might respond to targeted political messages. The company combined this with network analysis, exploiting friend connections and engagement metadata to build a comprehensive psychological map of the electorate. The New York Times emphasizes that Facebook’s algorithmic infrastructure enabled this misuse by allowing developers to collect not only users’ data but also that of their entire social networks, creating a massive dataset that could be repurposed for political manipulation (Confessore, 2018). This repurposing of data, gathered under the pretense of academic research, violated informed consent and exploited personal information for profit and political gain. The harm extended beyond individual privacy breaches; it undermined public trust, compromised user autonomy, and threatened the integrity of democratic processes. Facebook’s own rules required that data be used only for the purpose disclosed to users, yet in this case, that standard was clearly ignored.\n\nWho was being measured?\n\nThe individuals actually measured in this case were the relatively small group of Facebook users, about 270,000 people who installed Aleksandr Kogan’s personality quiz app “This Is Your Digital Life.” “These users consented to share their personal data for what they believed was academic research” (Wong, 2018). However, as The New York Times reported, the app also collected information from the users’ Facebook friends, expanding the dataset to more than 50 million profiles that had never agreed to participate (Confessore, 2018). In other words, “the people whose data was taken were not the people who had given permission” (Wong, 2018). Cambridge Analytica then attempted to generalize the personality insights drawn from this small, non-representative sample to the broader Facebook population, and, by extension, to the U.S. electorate. This extrapolation was both scientifically unsound and ethically problematic because it applied inferences from a limited dataset to millions of individuals who were never directly measured or informed.\n\nSummary: Why does this matter?\n\nThis case is uniquely pertinent, for it rests at the intersection of autonomy, democracy, and privacy. Those who benefited from the exploitation were data brokers, political strategists, and platform owners who exploited this breach for profit and influence (Wong, 2018). The individuals directly monitored were a small, consenting group of users who installed the quiz app. However, the harms transcended that sample. Millions of people who never consented to data collection had their personal information and “psychological traits inferred, modeled and used to shape and influence political behavior” (Confessore, 2018). As a result, both individual autonomy and democratic integrity were compromised through opaque micro-targeting and manipulation.\nSince the scandal broke, Facebook and Cambridge Analytica have faced extensive legal and reputational fallout. Facebook claimed it removed the “This Is Your Digital Life” app in 2015 and required that all parties certify the deletion of improperly obtained data. In a statement following the exposure, Facebook vice president Paul Grewal emphasized that the company was “committed to vigorously enforcing our policies to protect people’s information” and would take any necessary steps to ensure compliance (Confessore, 2018). However, these actions came only after the misuse had already occurred, raising questions about the adequacy of Facebook’s oversight and accountability mechanisms. Ultimately, the ethical violations in this case were driven more by profit and political gain than by transparency or public good. In a world where vast quantities of personal data can be harvested and repurposed in seconds, this episode serves as a cautionary example of the urgent need for stronger governance, informed consent standards, and ethical accountability in data-driven research and technology.\nimage source: The Guardian\nSources Cited:\n[Wong, Julia Carrie. “The Cambridge Analytica Scandal Changed the World – but It Didn’t Change Facebook.” The Guardian, The Guardian, 18 Mar. 2019] (www.theguardian.com/technology/2019/mar/17/the-cambridge-analytica-scandal-changed-the-world-but-it-didnt-change-facebook)\n[Confessore, Nicholas. “Cambridge Analytica and Facebook: The Scandal and the Fallout so Far.” Nytimes.com, The New York Times, 4 Apr. 2018,] (www.nytimes.com/2018/04/04/us/politics/cambridge-analytica-scandal-fallout.html)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thomas Price",
    "section": "",
    "text": "Hi! My name is Thomas Price and I am a current Senior at Pitzer College. At Pitzer, I study International Political Economy and Data Science. When I am not studying, you can find me playing tennis on our club tennis team, traveling around the country with the Claremont Debate Team or playing Pickle ball Tournaments!\n\nThe purpose of this website is the showcase projects that I have worked on in various data science classes, personal projects and a general portfolio of my interests.\n\nFeel free to surf around and check out some work, send me a note via email or peruse my linked-in, all linked below!"
  },
  {
    "objectID": "simulation.html",
    "href": "simulation.html",
    "title": "Understanding the Relationship Between GDP Per-Capita and Life Expectancy",
    "section": "",
    "text": "image source: Getty Images\n\n\nCode\n## Selecting data to be used\nlibrary(gapminder)\nlibrary(tidyverse)\nhead(gapminder)\n\n\n# A tibble: 6 × 6\n  country     continent  year lifeExp      pop gdpPercap\n  &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 Afghanistan Asia       1952    28.8  8425333      779.\n2 Afghanistan Asia       1957    30.3  9240934      821.\n3 Afghanistan Asia       1962    32.0 10267083      853.\n4 Afghanistan Asia       1967    34.0 11537966      836.\n5 Afghanistan Asia       1972    36.1 13079460      740.\n6 Afghanistan Asia       1977    38.4 14880372      786.\n\n\nCode\ngap2007 &lt;- gapminder |&gt;\n  filter(year == 2007) |&gt;\n  select(country, continent, lifeExp, gdpPercap)\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(scales)\n\nggplot(gap2007, aes(x = gdpPercap, y = lifeExp, color = continent)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  scale_x_continuous(\n    labels = label_number(scale = 1/1000, suffix = \"k\"),\n    name = \"GDP per Capita (thousands of US dollars)\"\n  ) +\n  labs(\n    title = \"GDP per Capita vs Life Expectancy (2007)\",\n    subtitle = \"Each point represents a country\",\n    y = \"Life Expectancy (years)\"\n  ) +\n  facet_wrap(~continent) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe scatter plot shows the observed relationship between life expectancy and GDP per capita across countries in 2007. There is a clear upward trend: countries with higher income levels tend to have longer life expectancy, although there is still substantial variation. Some countries achieve high life expectancy despite modest GDP per capita, and others show the opposite.This graph warrants testing whether the relationship is strong enough to be considered statistically meaningful.\nThis faceted graph shows the relationship between life expectancy and GDP per capita in 2007, separated by continent. Each point represents a country, and the line within each panel indicates the general direction of the relationship. Overall, continents like Europe and Asia show a strong positive correlation, while Africa and Oceania display weaker associations between life expectancy and GDP per capita.\n\n\nCode\n#Identifying the LifeExp slope with a linear model\n\nget_slope &lt;- function(df) {\n  fit &lt;- lm(gdpPercap ~ lifeExp, data = df)\n  coef(fit)[[\"lifeExp\"]]  \n}\n\n\nThe code above tests the slope of the simple regression. gdpPercap ~ lifeExp\nNull : beta1 = 0 Alt : beta1 ≠ 0\n\n\nCode\nobs_slopes &lt;- gap2007 |&gt;\n  group_by(continent) |&gt;\n  summarize(\n    obs_slope = coef(lm(gdpPercap ~ lifeExp))[2]\n  )\n\nobs_slopes\n\n\n# A tibble: 5 × 2\n  continent obs_slope\n  &lt;fct&gt;         &lt;dbl&gt;\n1 Africa         145.\n2 Americas      1293.\n3 Asia          1225.\n4 Europe        3366.\n5 Oceania       8972.\n\n\nCode\ncontinents &lt;- unique(gap2007$continent)\n\nobs_slopes_continent &lt;- numeric(length(continents))\n\nfor (i in 1:length(continents)) {\n  df_cont &lt;- subset(gap2007, continent == continents[i])\n  \n  obs_slopes_continent[i] &lt;- get_slope(df_cont)\n}\n\nnames(obs_slopes_continent) &lt;- continents\n\nobs_slope_overall &lt;- get_slope(gap2007)\nobs_slope_overall\n\n\n[1] 722.8975\n\n\nCode\nobs_slopes_continent\n\n\n     Asia    Europe    Africa  Americas   Oceania \n1225.2852 3365.9658  144.5327 1292.5570 8972.2195 \n\n\nThe above code uses a for loop to compute the observed slope for each continent. For each iteration, the code filtered the data set to a single continent, ran the get_slope() function and saved the slope value. This allows me to compare how the strength of the relationship between life expectancy and GDP per capita differs geographically.Given the slope output, we can predict the average change in GDP per capita (US$) for a 1 year increase in life. For example in Asia, we can predict 1 additional year of life yields 1225.2 additional gdp per capita.\nLarger positive slopes signify a steeper income to health relationship.\n\n\nCode\n# Permutation Test : Null Distribution\nperm_null &lt;- function(rep, data){\n  data |&gt;\n    group_by(continent) |&gt;\n    select(continent, lifeExp, gdpPercap) |&gt;\n    mutate(lifeExp_perm = sample(lifeExp, replace = FALSE)) |&gt;\n    summarize(\n      obs_slope  = coef(lm(gdpPercap ~ lifeExp      ))[2],\n      perm_slope = coef(lm(gdpPercap ~ lifeExp_perm ))[2],\n      .groups = \"drop\"\n    ) |&gt;\n    mutate(rep = rep)\n}\n\nset.seed(2025)\nB &lt;- 2000\n\n\nnull_by_cont_tbl &lt;- map(1:B, perm_null, data = gap2007) |&gt;\n  list_rbind()\n\nhead(null_by_cont_tbl)\n\n\n# A tibble: 6 × 4\n  continent obs_slope perm_slope   rep\n  &lt;fct&gt;         &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1 Africa         145.      -18.0     1\n2 Americas      1293.     -205.      1\n3 Asia          1225.     -155.      1\n4 Europe        3366.     -514.      1\n5 Oceania       8972.    -8972.      1\n6 Africa         145.       22.6     2\n\n\nTo simulate the null hypothesis that life expectancy and GDP per capita are unrelated, I created a permutation function. The function (perm_null) randomly shuffles life expectancy values within the dataset. For each randomization, a linear model is fit for each continent and the new slope is recorded. Repeating this 2,000 times using the map() function creates a null distribution of slopes that would occur purely by chance.\n\n\nCode\n#Calculating P-Values per continent \n\npvals_tbl &lt;- null_by_cont_tbl |&gt;\n  group_by(continent) |&gt;\n  summarize(\n    p_value = mean(abs(perm_slope) &gt;= abs(obs_slope)),\n    .groups = \"drop\"\n  )\n\npvals_tbl\n\n\n# A tibble: 5 × 2\n  continent p_value\n  &lt;fct&gt;       &lt;dbl&gt;\n1 Africa     0.004 \n2 Americas   0.007 \n3 Asia       0.0005\n4 Europe     0     \n5 Oceania    0.482 \n\n\nThe above table represents the null distribution through permutation. I calculated a two-sided p-value for each continent to test if the observed slope between GDP per capita and life expectancy was different from zero. The p-value above represents the proportion of permuted slopes whose absolute value was at least as large as the observed slope. This provides a measurement for how extreme the observed relationship is relative to what would be expected by complete chance. The results show quite small p-values for Africa, the Americas, Asia, and Europe. This indicates strong evidence of a real positive association. Oceania’s higher p-value suggests no statistically significant relationship, almost certainly due to its small sample size. Overall, the analysis suggests that while income and health are positively related worldwide, the strength of this relationship varies by region.\n\n\nCode\n#Plotting -- Faceted null distribution plots \n\nobs_slopes &lt;- gap2007 |&gt;\n  group_by(continent) |&gt;\n  summarize(obs_slope = coef(lm(gdpPercap ~ lifeExp))[2], .groups = \"drop\")\n\nggplot() +\n  geom_histogram(\n    data = null_by_cont_tbl,\n    aes(x = perm_slope),\n    bins = 40, fill = \"lightblue\", color = \"white\"\n  ) +  \n  geom_vline(\n    data = obs_slopes,\n    aes(xintercept = obs_slope),\n    color = \"red\", linewidth = .8\n  ) +\n  facet_wrap(~ continent, scales = \"free\") +\n  labs(\n    title = \"Permutation Test: Null Distribution of Slopes by Continent\",\n    subtitle = \"Red line = observed slope; histogram = null slopes (lifeExp shuffled)\",\n    x = \"Permuted slopes\", y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe histograms show slopes expected by chance if life expectancy and GDP were unrelated, while the red lines show the true slopes. In Asia, Europe, and the Americas, the observed slopes fall far outside the null distributions, indicating a strong real relationship. Africa shows a weaker but still significant effect. Oceania does not show significance because it includes only two countries, making its estimate unreliable.\nConclusion:\nThis analysis provides evidence that, in the broader population of countries represented in the 2007 Gapminder data, GDP per capita and life expectancy are positively associated. By using a permutation test, I generated a null distribution that represents what slopes would look like should the two variables be completely unrelated. Meaning that the act of generating GDP per capita was independent of life expectancy. I then compared the observed slopes to this null distribution by calculating two-sided p-values for each continent to assess whether the relationships observed could potentially have arisen by some random variation.\nThe results show statistically significant positive associations in the Americas, Africa, Asia, and Europe. Showing that higher income levels correspond to longer average lifespans. Oceania, with only two countries represented, does not indicate a significant relationship due to the insufficient data. These findings suggest that while economic development and public health are strongly linked globally, the strength of this connection depends on regional context. Importantly, the conclusions extend beyond the specific 2007 dataset. They provide evidence that, in the larger population of nations, life expectancy and GDP per capita are not independent but systematically related."
  },
  {
    "objectID": "images/Presentation.html#my-title",
    "href": "images/Presentation.html#my-title",
    "title": "Exploring the Use of Twitter Through Two Administrations",
    "section": "my title",
    "text": "my title\ns"
  },
  {
    "objectID": "images/Presentation.html#another",
    "href": "images/Presentation.html#another",
    "title": "Exploring the Use of Twitter Through Two Administrations",
    "section": "another",
    "text": "another\ngvjghv"
  },
  {
    "objectID": "Thesis.html",
    "href": "Thesis.html",
    "title": "Using R for Thesis Prospectus",
    "section": "",
    "text": "Graph 1 \nGraph 2 \nGraph 3"
  },
  {
    "objectID": "Presentation.html#introduction",
    "href": "Presentation.html#introduction",
    "title": "Analyzing Presidential Twitter Use: Obama vs. Trump",
    "section": "Introduction",
    "text": "Introduction\n\n \n\n\nReturn back to the main website through this link.\n\nGoal: \nAnalyze how Barack Obama and Donald Turmp used Twitter as communication tools during their presidencies.\n\nReturn back to the main website through this link."
  },
  {
    "objectID": "Presentation.html#approach",
    "href": "Presentation.html#approach",
    "title": "Analyzing Presidential Twitter Use: Obama vs. Trump",
    "section": "Approach",
    "text": "Approach\nStrategy: \n\nSift through Archives, and data repositories\nClean total amount of tweets\nMeasure what portion of tweets contained links, then what portion contained gov links\nCompare styles of communication between administrations\n\n\n\n\n\nRaw tweet data before cleaning\n\n\n\n\nReturn back to the main website through this link."
  },
  {
    "objectID": "Presentation.html#obamas-twitter-usage",
    "href": "Presentation.html#obamas-twitter-usage",
    "title": "Analyzing Presidential Twitter Use: Obama vs. Trump",
    "section": "Obama’s Twitter Usage",
    "text": "Obama’s Twitter Usage\n\n  \n\nKey Findings - Joined Twitter in 2015, posting 321 tweets through his entire administration\n- 42% contained links\n- 12.5% directed users to government websites\n- Common domains: whitehouse.gov, healthcare.gov, vote.gov\n\nReturn back to the main website through this link."
  },
  {
    "objectID": "Presentation.html#trumps-twitter-usage",
    "href": "Presentation.html#trumps-twitter-usage",
    "title": "Analyzing Presidential Twitter Use: Obama vs. Trump",
    "section": "Trumps Twitter Usage",
    "text": "Trumps Twitter Usage\n\n\n\n\nSummary of Trump’s tweet volume and link usage\n\n\n\n\n\nTweets containing .gov links during Trump’s first term\n\n\n\n\n\nReturn back to the main website through this link.\n\nKey Findings - Tweeted over 23,000 times during his first term\n- About 50% of tweets contained at least one link\n- Only 5 tweets (0.02%) linked to .gov sites\n- Most links pointed to media outlets or campaign-related pages, not government resources\nAnalysis\nTrump used Twitter as a personal broadcast and media amplifier, rather than as a channel for directing people to official government information.\n\nReturn back to the main website through this link."
  },
  {
    "objectID": "Presentation.html#comparing-obama-and-trumps-twitter-usage",
    "href": "Presentation.html#comparing-obama-and-trumps-twitter-usage",
    "title": "Analyzing Presidential Twitter Use: Obama vs. Trump",
    "section": "Comparing Obama and Trump’s Twitter Usage",
    "text": "Comparing Obama and Trump’s Twitter Usage\n\n \n\n\nKey Comparison Findings\n- Obama: 321 tweets, 42% contained links, 12.5% led to .gov sites\n- Trump: 23,000 tweets, 50% contained links, only 0.02% led to .gov sites\n- Obama’s communication emphasized official information and public resources\n- Trump’s Twitter focused on personal messaging, media reactions, and direct engagement\n\nReturn back to the main website through this link."
  },
  {
    "objectID": "Presentation.html#thank-you",
    "href": "Presentation.html#thank-you",
    "title": "Analyzing Presidential Twitter Use: Obama vs. Trump",
    "section": "Thank You!",
    "text": "Thank You!\nSources:\nRefrence 1 : The data used in this analysis comes from the National Archives, which maintains a full record of Barack Obama’s tweets during his presidency. The dataset was downloaded as a CSV file and stored locally for analysis.\nRefrence 2: Mark Hershey compiled the dataset of Trump’s Tweets and they are stored in his github linked below. Original Dataset for Trumps Tweets\nOriginal Source: “Archived White House Websites and Social Media | Barack Obama Presidential Library.” Obamalibrary.gov, 2017, www.obamalibrary.gov/digital-research-room/archived-white-house-websites-and-social-media. Accessed 4 Dec. 2025.\n“CompleteTrumpTweetsArchive/Data/RealDonaldTrump_in_office.csv at Master · MarkHershey/CompleteTrumpTweetsArchive.” GitHub, 2025, github.com/MarkHershey/CompleteTrumpTweetsArchive/blob/master/data/realDonaldTrump_in_office.csv. Accessed 4 Dec. 2025.\n\nReturn back to the main website through this link."
  },
  {
    "objectID": "billboard_t100.html",
    "href": "billboard_t100.html",
    "title": "Billboard Hot 100 Number Ones",
    "section": "",
    "text": "View Original Data Set Here\n\n\nCode\nlibrary(tidyverse)\n\nbillboard &lt;- readr::read_csv(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-26/billboard.csv\"\n)\n\nartist_counts &lt;- billboard |&gt;\n  group_by(artist) |&gt;\n  summarise(n = n()) |&gt;\n  arrange(desc(n))\n\ntop10_artists &lt;- head(artist_counts, 10)\n\n\nggplot(top10_artists, aes(x = reorder(artist, n), y = n)) +\n  geom_col(fill = \"steelblue\") +\n  geom_text(aes(label = n), hjust = 1.2, color = \"white\", size = 4) +\n  coord_flip() +   # flips so artists are readable\n  labs(\n    title = \"Top 10 Artists by Number of Hot 100 Songs\",\n    x = \"Artist\",\n    y = \"Number of Songs\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe figure above highlights the artists with the most Billboard Hot 100 songs between 1958 and 2025. The Beatles lead with 20 entries, followed by Mariah Carey and Madonna. The chart showcases how a small minority of influential artists have dominated popular music across decades. Overall, it dictates the lasting impact of these artists and illistrates the evolution of popular music over time.\nimage source: Wikipedia"
  },
  {
    "objectID": "Proj2.html",
    "href": "Proj2.html",
    "title": "Analysis of Obama’s Tweets",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(ggrepel)\n\n# Reading the data\nobama &lt;- read_csv(\"Obama_Tweets/tweets.csv\")\n\n# Cleaning data\ntweets &lt;- obama |&gt;\n  filter(!is.na(text)) |&gt; \n  mutate(\n    text_lower = str_to_lower(text),\n    text_noline = str_replace_all(text_lower, \"[\\r\\n]\", \" \")\n  )\n\nhead(tweets)\n\n\n# A tibble: 6 × 12\n  tweet_id in_reply_to_status_id in_reply_to_user_id timestamp      source text \n     &lt;dbl&gt;                 &lt;dbl&gt;               &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;  &lt;chr&gt;\n1  7.97e17                    NA                  NA 2016-11-11 17… \"&lt;a h… Toda…\n2  7.96e17                    NA                  NA 2016-11-08 15… \"&lt;a h… Toda…\n3  7.94e17                    NA                  NA 2016-11-03 23… \"&lt;a h… It's…\n4  7.94e17                    NA                  NA 2016-11-03 06… \"&lt;a h… It h…\n5  7.90e17                    NA                  NA 2016-10-23 17… \"&lt;a h… I'll…\n6  7.90e17                    NA                  NA 2016-10-21 22… \"&lt;a h… Chec…\n# ℹ 6 more variables: retweeted_status_id &lt;dbl&gt;,\n#   retweeted_status_user_id &lt;dbl&gt;, retweeted_status_timestamp &lt;chr&gt;,\n#   expanded_urls &lt;chr&gt;, text_lower &lt;chr&gt;, text_noline &lt;chr&gt;\n\n\nThe table above uses data from the National Archives containing Obama’s tweets during his presidency. He posted a total of 325 tweets over his two-year tenure on Twitter. The next step in the analysis involves cleaning the data to better understand the nature of his tweets, with a focus on those containing links. This table displays each tweet’s date, source, text, retweet status, and any associated URLs. The cleaning process removes rows with missing text and standardizes the remaining data by converting all text to lowercase and eliminating line breaks.\n\n\nCode\n# Identifying tweets that contain links and government links. \n\n\nlibrary(tidyverse)\nlibrary(stringr)\n\ntweets_table &lt;- tweets |&gt;\n  mutate(\n    created_at = timestamp,\n    has_link = !is.na(expanded_urls),\n    gov_link = str_detect(expanded_urls,\n    \"(?&lt;=https?://)[^\\\\s]*\\\\.gov\"), \n    has_http_in_text = str_detect(text, \"https?://[^\\\\s]+\")) |&gt;\n  select(created_at, text, has_link, gov_link, has_http_in_text)\n\n\n\n\nCode\n#Creating a new table with those links \ntweets_table |&gt;\n  summarise(\n    total_tweets = n(),\n    with_links = sum(has_link),\n    pct_with_links = mean(has_link),\n    pct_gov_of_links = mean(gov_link[has_link], na.rm = TRUE)\n  )\n\n\n# A tibble: 1 × 4\n  total_tweets with_links pct_with_links pct_gov_of_links\n         &lt;int&gt;      &lt;int&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1          321        135          0.421            0.126\n\n\nThis table showcases the total number of tweets that Obama posted during his short Twitter tenure. The table highlights tweets that contained links, and further identifies the percentage of those links that directed users to government websites. We can see that there were a total of 321 tweets, 135 of which contained links (42%). Among those tweets, 12.5% included links to government websites.\n\n\nCode\n#Visualizing which tweets contained links vs Gov links\n\nlibrary(lubridate)\nlibrary(ggplot2)\n\nlinks_summary &lt;- tweets_table |&gt;\n  summarise(\n    with_links = sum(has_link, na.rm = TRUE),\n    with_gov_links = sum(gov_link, na.rm = TRUE)\n  ) |&gt;\n  pivot_longer(cols = everything(), names_to = \"type\", values_to = \"count\")\n\nggplot(links_summary, aes(x = type, y = count, fill = type)) +\n  geom_col(width = 0.5) +\n  labs(\n    title = \"Obama Tweets Containing .Gov and Non .Gov Links\",\n    x = NULL,\n    y = \"Number of Tweets\"\n  ) +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"steelblue\", \"darkred\"))\n\n\n\n\n\n\n\n\n\nThe graph above visualizes the data from the previous table, showing the total number of tweets that contained links and the subset that included government links. Only a small percentage of tweets directed users to .gov websites. Since Twitter is a relatively new platform for sharing official information, it would be interesting to compare how more recent administrations have used it to promote government resources and public communication.\n\n\nCode\ntweets_table &lt;- tweets |&gt;\n  mutate(\n    created_at = timestamp,\n    has_link = !is.na(expanded_urls),\n    gov_link = str_detect(expanded_urls, \"(?&lt;=https?://)[^\\\\s]*\\\\.gov\"),\n    has_http_in_text = str_detect(text, \"https?://[^\\\\s]+\")\n  ) |&gt;\n  select(created_at, text, expanded_urls, has_link, gov_link, has_http_in_text)\n\n\nThe code above isolates all of Obama’s tweets that contain .gov links and extracts the specific government domain from each URL. It then counts how many times each unique domain appears to identify which sites he referenced most frequently. The resulting table shows that Obama’s tweets most often linked to official government pages such as whitehouse.gov, emphasizing his use of Twitter to share verified information and policy resources.\n\n\nCode\nlibrary(stringr)\nlibrary(dplyr)\n\ngov_domains &lt;- tweets_table |&gt;\n  filter(gov_link, !is.na(expanded_urls)) |&gt;\n  \n  mutate(urls = strsplit(expanded_urls, \"\\\\s+\")) |&gt;   # If a tweet has several URLs, split them so each is counted\n  tidyr::unnest(urls) |&gt;\n  mutate(domain = str_extract(urls, \"(?&lt;=://)[^/]+\"),\n         domain = str_remove(domain, \"^www\\\\.\")) |&gt;\n  filter(str_detect(domain, \"\\\\.gov$\")) |&gt;\n  count(domain, sort = TRUE)\n\nhead(gov_domains, 10)\n\n\n# A tibble: 10 × 2\n   domain              n\n   &lt;chr&gt;           &lt;int&gt;\n 1 go.wh.gov           4\n 2 1.usa.gov           1\n 3 AidRefugees.gov     1\n 4 HealthCare.gov      1\n 5 Healthcare.gov      1\n 6 Ready.gov           1\n 7 fafsa.gov           1\n 8 go.nasa.gov         1\n 9 vote.gov            1\n10 vote.usa.gov        1\n\n\nThis table lists the government websites most frequently linked in Obama’s tweets. The majority of links directed users to go.wh.gov, a shortened White House domain used to share official announcements and press materials. Other links point to various federal resources such as HealthCare.gov, FAFSA.gov, and Vote.gov, reflecting Obama’s efforts to promote public programs and civic engagement through his social media presence.\nReading and Cleaning Trumps Tweets:\n\n\nCode\nlibrary(tidyverse)\n\n# Read the CSV from GitHub\ntrump &lt;- read_csv(\"https://raw.githubusercontent.com/MarkHershey/CompleteTrumpTweetsArchive/refs/heads/master/data/realDonaldTrump_in_office.csv\")\n\n\nhead(trump)\n\n\n# A tibble: 6 × 4\n  ID               Time                `Tweet URL`                  `Tweet Text`\n  &lt;chr&gt;            &lt;dttm&gt;              &lt;chr&gt;                        &lt;chr&gt;       \n1 @realDonaldTrump 2017-01-20 06:31:00 https://twitter.com/realDon… It all begi…\n2 @realDonaldTrump 2017-01-20 11:51:00 https://twitter.com/realDon… Today we ar…\n3 @realDonaldTrump 2017-01-20 11:51:00 https://twitter.com/realDon… power from …\n4 @realDonaldTrump 2017-01-20 11:52:00 https://twitter.com/realDon… What truly …\n5 @realDonaldTrump 2017-01-20 11:53:00 https://twitter.com/realDon… January 20t…\n6 @realDonaldTrump 2017-01-20 11:54:00 https://twitter.com/realDon… The forgott…\n\n\n\n\nCode\nnames(trump)\n\n\n[1] \"ID\"         \"Time\"       \"Tweet URL\"  \"Tweet Text\"\n\n\nRe-naming the columns of Trumps tweets so they match that of Obama’s\n\n\nCode\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(lubridate)\n\ntrump_clean &lt;- trump |&gt;\n  rename(\n    id        = ID,\n    timestamp = Time,\n    tweet_url = `Tweet URL`,\n    text      = `Tweet Text`\n  )\n\nhead(trump_clean)\n\n\n# A tibble: 6 × 4\n  id               timestamp           tweet_url                           text \n  &lt;chr&gt;            &lt;dttm&gt;              &lt;chr&gt;                               &lt;chr&gt;\n1 @realDonaldTrump 2017-01-20 06:31:00 https://twitter.com/realDonaldTrum… It a…\n2 @realDonaldTrump 2017-01-20 11:51:00 https://twitter.com/realDonaldTrum… Toda…\n3 @realDonaldTrump 2017-01-20 11:51:00 https://twitter.com/realDonaldTrum… powe…\n4 @realDonaldTrump 2017-01-20 11:52:00 https://twitter.com/realDonaldTrum… What…\n5 @realDonaldTrump 2017-01-20 11:53:00 https://twitter.com/realDonaldTrum… Janu…\n6 @realDonaldTrump 2017-01-20 11:54:00 https://twitter.com/realDonaldTrum… The …\n\n\nCleaning the Tweets: making them all lowercase and removing line breaks\n\n\nCode\ntrump_tweets &lt;- trump_clean |&gt;\n  filter(!is.na(text)) |&gt;\n  mutate(\n    text_lower  = str_to_lower(text),\n    text_noline = str_replace_all(text_lower, \"[\\r\\n]\", \" \")\n  )\n\nhead(trump_tweets)\n\n\n# A tibble: 6 × 6\n  id               timestamp           tweet_url    text  text_lower text_noline\n  &lt;chr&gt;            &lt;dttm&gt;              &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;      &lt;chr&gt;      \n1 @realDonaldTrump 2017-01-20 06:31:00 https://twi… It a… it all be… it all beg…\n2 @realDonaldTrump 2017-01-20 11:51:00 https://twi… Toda… today we … today we a…\n3 @realDonaldTrump 2017-01-20 11:51:00 https://twi… powe… power fro… power from…\n4 @realDonaldTrump 2017-01-20 11:52:00 https://twi… What… what trul… what truly…\n5 @realDonaldTrump 2017-01-20 11:53:00 https://twi… Janu… january 2… january 20…\n6 @realDonaldTrump 2017-01-20 11:54:00 https://twi… The … the forgo… the forgot…\n\n\nCreating a table with gov links\n\n\nCode\ntrump_tweets_table &lt;- trump_tweets |&gt;\n  mutate(\n    created_at = timestamp,\n    has_link   = str_detect(text, \"https?://[^\\\\s]+\"),\n    gov_link   = str_detect(text, \"(?&lt;=https?://)[^\\\\s]*\\\\.gov\")\n  ) |&gt;\n  select(created_at, text, has_link, gov_link)\n\nhead(trump_tweets_table)\n\n\n# A tibble: 6 × 4\n  created_at          text                                     has_link gov_link\n  &lt;dttm&gt;              &lt;chr&gt;                                    &lt;lgl&gt;    &lt;lgl&gt;   \n1 2017-01-20 06:31:00 It all begins today! I will see you at … FALSE    FALSE   \n2 2017-01-20 11:51:00 Today we are not merely transferring po… FALSE    FALSE   \n3 2017-01-20 11:51:00 power from Washington, D.C. and giving … FALSE    FALSE   \n4 2017-01-20 11:52:00 What truly matters is not which party c… FALSE    FALSE   \n5 2017-01-20 11:53:00 January 20th 2017, will be remembered a… FALSE    FALSE   \n6 2017-01-20 11:54:00 The forgotten men and women of our coun… FALSE    FALSE   \n\n\nSummary of the table that shows total amount of tweets containing gov links\n\n\nCode\ntrump_summary &lt;- trump_tweets_table |&gt;\n  summarise(\n    total_tweets     = n(),\n    with_links       = sum(has_link),\n    pct_with_links   = mean(has_link),\n    pct_gov_of_links = mean(gov_link[has_link], na.rm = TRUE)\n  )\n\ntrump_summary\n\n\n# A tibble: 1 × 4\n  total_tweets with_links pct_with_links pct_gov_of_links\n         &lt;int&gt;      &lt;int&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1        23073      11352          0.492         0.000440\n\n\nThe table above shows the total number of tweets that Trump posted during his first term. Crucially, it shows the percentage that contained links (50%) and, of those, how many directed users to government websites (0.0004%).\nExtracting URL’s from the tweets\n\n\nCode\nlibrary(tidyverse)\nlibrary(stringr)\n\ntrump_links &lt;- trump_tweets_table |&gt;\n  filter(has_link) |&gt;\n  mutate(\n    urls = str_extract_all(text, \"https?://[^\\\\s]+\")  # extract all URLs\n  ) |&gt;\n  unnest(urls)  \n\nhead(trump_links)\n\n\n# A tibble: 6 × 5\n  created_at          text                               has_link gov_link urls \n  &lt;dttm&gt;              &lt;chr&gt;                              &lt;lgl&gt;    &lt;lgl&gt;    &lt;chr&gt;\n1 2017-01-20 11:58:00 It is time to remember that...htt… TRUE     FALSE    http…\n2 2017-01-20 12:00:00 So to all Americans, in every cit… TRUE     FALSE    http…\n3 2017-01-20 12:13:00 TO ALL AMERICANS https://www.face… TRUE     FALSE    http…\n4 2017-01-25 18:03:00 Beginning today, the United State… TRUE     FALSE    http…\n5 2017-01-26 17:53:00 Miami-Dade Mayor drops sanctuary … TRUE     FALSE    http…\n6 2017-01-27 10:30:00 . @ VP Mike Pence will be speakin… TRUE     FALSE    http…\n\n\nIsolating Gov Links\n\n\nCode\ntrump_gov_links &lt;- trump_links |&gt;\n  filter(str_detect(urls, \"(?&lt;=https?://)[^\\\\s]*\\\\.gov\")) |&gt;\n  mutate(\n    domain = str_extract(urls, \"(?&lt;=://)[^/]+\"),    \n    domain = str_remove(domain, \"^www\\\\.\")          \n  )\n\nhead(trump_gov_links)\n\n\n# A tibble: 5 × 6\n  created_at          text                        has_link gov_link urls  domain\n  &lt;dttm&gt;              &lt;chr&gt;                       &lt;lgl&gt;    &lt;lgl&gt;    &lt;chr&gt; &lt;chr&gt; \n1 2017-01-27 14:20:00 Statement on International… TRUE     TRUE     http… white…\n2 2020-01-27 11:09:00 On International Holocaust… TRUE     TRUE     http… white…\n3 2020-03-11 14:17:00 I want to thank all of our… TRUE     TRUE     http… CDC.g…\n4 2020-04-23 10:14:00 https://www.whitehouse.gov… TRUE     TRUE     http… white…\n5 2020-05-20 13:34:00 https://www.whitehouse.gov… TRUE     TRUE     http… white…\n\n\nVisualizing top government domains\n\n\nCode\ntrump_gov_domains &lt;- trump_gov_links |&gt;\n  count(domain, sort = TRUE)\n\ntrump_gov_domains\n\n\n# A tibble: 2 × 2\n  domain             n\n  &lt;chr&gt;          &lt;int&gt;\n1 whitehouse.gov     4\n2 CDC.gov            1\n\n\nComparing Trump’s Twitter usage to Obama’s\n\n\nCode\n# Combine both presidents' summaries\n\n# Obama counts\nobama_counts &lt;- tweets_table |&gt;\n  summarise(\n    total_tweets = n(),\n    gov_links    = sum(gov_link, na.rm = TRUE)\n  ) |&gt;\n  mutate(president = \"Obama\")\n\n# Trump counts\ntrump_counts &lt;- trump_tweets_table |&gt;\n  summarise(\n    total_tweets = n(),\n    gov_links    = sum(gov_link, na.rm = TRUE)\n  ) |&gt;\n  mutate(president = \"Trump\")\n\n# Combine into one table\ncomparison_counts &lt;- bind_rows(obama_counts, trump_counts) |&gt;\n  select(president, total_tweets, gov_links)\n\ncomparison_counts &lt;- comparison_counts |&gt;\n  mutate(\n    pct_gov_links = (gov_links / total_tweets) * 100\n  )\n\ncomparison_counts\n\n\n# A tibble: 2 × 4\n  president total_tweets gov_links pct_gov_links\n  &lt;chr&gt;            &lt;int&gt;     &lt;int&gt;         &lt;dbl&gt;\n1 Obama              321        17        5.30  \n2 Trump            23073         5        0.0217\n\n\nPlotting the percentage delta\n\n\nCode\nlibrary(ggplot2)\n\nggplot(comparison_counts, aes(x = president, y = gov_links, fill = president)) +\n  geom_col(width = 0.6) +\n  geom_text(aes(label = paste0(round(pct_gov_links, 2), \"%\")),\n            vjust = -0.5, size = 4) +\n  labs(\n    title = \"Tweets Containing .Gov Links as a Share of Total Tweets\",\n    x = NULL,\n    y = \"Number of Tweets\"\n  ) +\n  scale_fill_manual(values = c(\"steelblue\", \"darkred\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nDonald Trump’s use of Twitter is vastly different from that of his predecessors. Over the course of his first term (2016 - 2020), Trump tweeted more than 23,000 times, often using the platform as his main form of disseminating information to the public. However, among those 23,000 tweets, only 5 contained direct links to government resources (0.02%).\nIn comparison to Obama, who used Twitter as a vehicle to distribute institutional communication, often linking verified government resources like WhiteHouse.gov or HealthCare.gov. Trump’s feed was often comprised of commentary, reactions, and amplification of external media sources. His use of the platform reflected his style of immediacy and opinion, directly engaging with his audience rather than supplying vital information during a crisis like COVID.\nThe quantitative difference between the two reflects a broader stylistic and strategic contrast. Obama used his Twitter as a strategic governance tool, and Trump used it as a personal broadcasting platform. Despite the absolute prolific amount of tweets, the near absence of .gov links showcases a shift from institutional communication to personal rhetoric, which in turn became a defining attribute of his administration.\nOriginal Source: “Archived White House Websites and Social Media | Barack Obama Presidential Library.” Obamalibrary.gov, 2017, www.obamalibrary.gov/digital-research-room/archived-white-house-websites-and-social-media. Accessed 4 Dec. 2025.\n“CompleteTrumpTweetsArchive/Data/RealDonaldTrump_in_office.csv at Master · MarkHershey/CompleteTrumpTweetsArchive.” GitHub, 2025, github.com/MarkHershey/CompleteTrumpTweetsArchive/blob/master/data/realDonaldTrump_in_office.csv. Accessed 4 Dec. 2025."
  },
  {
    "objectID": "Proj5.html",
    "href": "Proj5.html",
    "title": "Racial Disparities in Police Search Practices Across Three California Cities",
    "section": "",
    "text": "View Original Data Set Here\nReference: A large-scale analysis of racial disparities in police stops across the United States, Nature Human Behaviour\nOriginal Author: Pierson, E., Simoiu, C., Overgoor, J., Corbett-Davies, S., Ramachandran, V., Phillips, C., Shroff, R., & Goel, S. (2020).\n\n\nCode\nlibrary(DBI)\nlibrary(RMariaDB)\n\ncon_traffic &lt;- dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"traffic\",\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n)\n\n\nViewing SF lables\n\n\nCode\nSHOW COLUMNS FROM ca_san_francisco_2020_04_01;\n\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nraw_row_number\ntext\nYES\n\nNA\n\n\n\ndate\ndate\nYES\n\nNA\n\n\n\ntime\ntime\nYES\n\nNA\n\n\n\nlocation\ntext\nYES\n\nNA\n\n\n\nlat\ndouble\nYES\n\nNA\n\n\n\nlng\ndouble\nYES\n\nNA\n\n\n\ndistrict\ntext\nYES\n\nNA\n\n\n\nsubject_age\nbigint(20)\nYES\n\nNA\n\n\n\nsubject_race\ntext\nYES\n\nNA\n\n\n\nsubject_sex\ntext\nYES\n\nNA\n\n\n\n\n\n\nViewing LA labels\n\n\nCode\nSHOW COLUMNS FROM ca_los_angeles_2020_04_01;\n\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nraw_row_number\ntext\nYES\n\nNA\n\n\n\ndate\ndate\nYES\n\nNA\n\n\n\ntime\ntime\nYES\n\nNA\n\n\n\ndistrict\ntext\nYES\n\nNA\n\n\n\nregion\ntext\nYES\n\nNA\n\n\n\nsubject_race\ntext\nYES\n\nNA\n\n\n\nsubject_sex\ntext\nYES\n\nNA\n\n\n\nofficer_id_hash\ntext\nYES\n\nNA\n\n\n\ntype\ntext\nYES\n\nNA\n\n\n\nraw_descent_description\ntext\nYES\n\nNA\n\n\n\n\n\n\nViewing San Diego Lables\n\n\nCode\nSHOW COLUMNS FROM ca_san_diego_2020_04_01;\n\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nraw_row_number\ntext\nYES\n\nNA\n\n\n\ndate\ndate\nYES\n\nNA\n\n\n\ntime\ntime\nYES\n\nNA\n\n\n\nservice_area\ntext\nYES\n\nNA\n\n\n\nsubject_age\nbigint(20)\nYES\n\nNA\n\n\n\nsubject_race\ntext\nYES\n\nNA\n\n\n\nsubject_sex\ntext\nYES\n\nNA\n\n\n\ntype\ntext\nYES\n\nNA\n\n\n\narrest_made\ndouble\nYES\n\nNA\n\n\n\ncitation_issued\ndouble\nYES\n\nNA\n\n\n\n\n\n\nViewing San Jose\n\n\nCode\nDESCRIBE ca_san_jose_2020_04_01;\n\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nraw_row_number\ntext\nYES\n\nNA\n\n\n\ndate\ndate\nYES\n\nNA\n\n\n\ntime\ntime\nYES\n\nNA\n\n\n\nlocation\ntext\nYES\n\nNA\n\n\n\nlat\ndouble\nYES\n\nNA\n\n\n\nlng\ndouble\nYES\n\nNA\n\n\n\nsubject_race\ntext\nYES\n\nNA\n\n\n\ntype\ntext\nYES\n\nNA\n\n\n\narrest_made\ndouble\nYES\n\nNA\n\n\n\ncitation_issued\ndouble\nYES\n\nNA\n\n\n\n\n\n\nViewing SF, SD, and SJ search & hit rates by race\n\n\nCode\nSELECT \n    'San Francisco' AS city,\n    subject_race,\n    COUNT(*) AS n_stops,\n    AVG(search_conducted = 1) AS search_rate,\n    AVG(CASE WHEN search_conducted = 1 THEN contraband_found = 1 END) AS hit_rate\nFROM ca_san_francisco_2020_04_01\nWHERE subject_race IS NOT NULL\nGROUP BY subject_race\n\nUNION ALL\n\nSELECT \n    'San Diego' AS city,\n    subject_race,\n    COUNT(*) AS n_stops,\n    AVG(search_conducted = 1) AS search_rate,\n    AVG(CASE WHEN search_conducted = 1 THEN contraband_found = 1 END) AS hit_rate\nFROM ca_san_diego_2020_04_01\nWHERE subject_race IS NOT NULL\nGROUP BY subject_race\n\nUNION ALL\n\nSELECT \n    'San Jose' AS city,\n    subject_race,\n    COUNT(*) AS n_stops,\n    AVG(search_conducted = 1) AS search_rate,\n    AVG(CASE WHEN search_conducted = 1 THEN contraband_found = 1 END) AS hit_rate\nFROM ca_san_jose_2020_04_01\nWHERE subject_race IS NOT NULL\nGROUP BY subject_race\n\nORDER BY city, subject_race;\n\n\n\n\nCode\nrates\n\n\n            city           subject_race n_stops search_rate hit_rate\n1      San Diego asian/pacific islander   32541      0.0280   0.1044\n2      San Diego                  black   42705      0.0907   0.0932\n3      San Diego               hispanic  117083      0.0555   0.0812\n4      San Diego                  other   27238      0.0166   0.0953\n5      San Diego                  white  162226      0.0278   0.1149\n6  San Francisco asian/pacific islander  157684      0.0181   0.3611\n7  San Francisco                  black  152196      0.1552   0.0924\n8  San Francisco               hispanic  116014      0.0987   0.1018\n9  San Francisco                  other  106858      0.0351   0.2032\n10 San Francisco                  white  372318      0.0314   0.2421\n11      San Jose asian/pacific islander   16062      0.1352   0.1800\n12      San Jose                  black   13538      0.3280   0.1867\n13      San Jose               hispanic   79885      0.3420   0.1571\n14      San Jose                  other   11523      0.1768   0.2274\n15      San Jose                  white   26341      0.2454   0.1984\n\n\nCleaning the data in order to visualize\n\n\nCode\nlibrary(tidyverse)\n\nrates_clean &lt;- rates |&gt;\n  mutate(\n    search_rate = round(search_rate * 100, 1),\n    hit_rate    = round(hit_rate * 100, 1)\n  )\n\n\nVisualization 1: Search Rates by Race\n\n\nCode\nggplot(rates_clean, aes(x = subject_race, y = search_rate, fill = city)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Search Rates by Race Across Three California Cities\",\n    x = \"Race\",\n    y = \"Search Rate (%)\",\n    fill = \"City\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nThe figure above compares the likelihood that officers searched drivers from different racial groups across the three cities. What is clear is that the search rate varies significantly by race. In all three cities, Latino and Black drivers were searched at significantly higher rates than White and Asian drivers. These results help suggest that the decision to stop and search drivers is not uniformly applied across racial groups. Furthermore, the data seems skewed because San Jose has higher hit rates across all racial groups. This does not mean that individuals in San Jose are more likely to merit searches; instead, it reflects systematic differences in policing practices and reporting standards. San Jose likely documents searches more frequently and consistently or, more plausibly, operates under a system that has policies to increase the number of searches per stop.\nVisualization 2: Hit rates of contraband found by race\n\n\nCode\nggplot(rates_clean, aes(x = subject_race, y = hit_rate, fill = city)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Contraband Hit Rates by Race Across Three California Cities\",\n    x = \"Race\",\n    y = \"Hit Rate (%)\",\n    fill = \"City\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nThe contribution hit rate seen above shows significant differences in search effectiveness across the three cities. San Francisco consistently has the highest hit rates across racial groups, which suggests that its officers conduct fewer but far more targeted searches. San Jose conducts far more searches but has only mid-level hit rates, indicating a lower threshold for initiating searches that frequently do not result in contraband. San Diego has the lowest hit rates overall, which implies that many searches in the city are not based on strong suspicion. As a takeaway, it seems that San Francisco’s searches are far more effective, San Jose’s are more frequent but less productive, and San Diego’s are both infrequent and largely unsuccessful. Overall, this plot highlights how policing strategies are highly varied and city-dependent, even when they all operate in the same state.\nVisualization 3\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\n\nrates_eff &lt;- rates |&gt;\n  mutate(\n    search_rate_pct = search_rate * 100,\n    hit_rate_pct = hit_rate * 100\n  )\n\nggplot(rates_eff, aes(x = search_rate_pct, \n                      y = hit_rate_pct, \n                      color = city, \n                      size = n_stops, \n                      label = subject_race)) +\n  geom_point(alpha = 0.8) +\n  geom_text(vjust = -0.8, size = 3, show.legend = FALSE) +\n  labs(\n    title = \"Search Efficiency: Search Rates vs Hit Rates Across Racial Groups\",\n    subtitle = \"Each point sized by number of stops; labels represent racial groups\",\n    x = \"Search Rate (%)\",\n    y = \"Hit Rate (%)\",\n    color = \"City\",\n    size = \"Number of Stops\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 14, face = \"bold\"))\n\n\n\n\n\n\n\n\n\nFor this visualization, I was interested in exploring whether some groups are searched more but found with contraband less often. On the contrary, there are some groups that are searched less but found with contraband more. The goal was to help unpack biases that officers may have. The plot compares how different racial groups are searched and how often those searches actually find contributors. San Diego shows both low search rates and low hit rates across all racial groups, which suggests fewer searches overall but simultaneously less efficiency. San Francisco, by contrast, shows moderate search rates paired with higher hit rates, which indicates more targeted, successful searches. San Jose consistently has the highest search rates but only mid-level hit rates. This implies that officers search more often but are way less efficient.\nConclusion-\nAcross San Francisco, San Diego and San Jose, the data above highlights clear racial disparities when it comes to the frequency of drivers being searched and how effective those searches are. Cities have stark differences in regards to search intensity as well as efficiency. San Francisco for example, conducts fewer yet more targeted searches. Where San Jose searches far more often but with lower sucess rates.\nTogather, these results help suggest that policing stratagies and potential biases vary substantially across police jurisdictions. This helps underscore the need for consistant statewide oversight that involves data driven policy reform.\nSources Cited:\nPierson, Emma, et al. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 2020. Data from: Stanford Open Policing Project (https://openpolicing.stanford.edu ).\nimage source: OpenPolicing\n\n\nCode\nDBI::dbDisconnect(con_traffic, shutdown = TRUE)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Thomas Price, a senior at Pitzer College majoring in International Political Economy and minoring in Data Science. I recently studied abroad in Spain, and the photo above is from a trip my friends and I took to Monte Carlo! The drop-down menu has links to work I have done. This work ranges from basic data visualization, permutation tests, web-scraping and merging data sets using SQL."
  },
  {
    "objectID": "about.html#bio",
    "href": "about.html#bio",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Thomas Price, a senior at Pitzer College majoring in International Political Economy and minoring in Data Science. I recently studied abroad in Spain, and the photo above is from a trip my friends and I took to Monte Carlo! The drop-down menu has links to work I have done. This work ranges from basic data visualization, permutation tests, web-scraping and merging data sets using SQL."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\n\nSummer Analyst at AlphaSights (San Francisco)\n\nReturning full time in Summer of 2026\n\nStrategy Intern at BCIU (Washington,DC)\nLegislative Intern at the US House of Representatives (Washington, DC)\nDevelopment Intern at the Institute for Economics and Peace (UN organization)"
  },
  {
    "objectID": "about.html#projects",
    "href": "about.html#projects",
    "title": "About",
    "section": "Projects",
    "text": "Projects\n\nSenior thesis: Analyzing Chinese Soft Power and Economic Statecraft through the Belt & Road Initiative\n\nData wrangling/visualization with R + ggplot"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About",
    "section": "Contact",
    "text": "Contact\nFeel free to connect with me: LinkedIn | GitHub | Email | Resume"
  },
  {
    "objectID": "tidygas.html",
    "href": "tidygas.html",
    "title": "US Gas Prices",
    "section": "",
    "text": "View the original TidyTuesday dataset\n\n\nCode\nlibrary(tidyverse)\n\nweekly_gas_prices &lt;- readr::read_csv(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-01/weekly_gas_prices.csv\",\n  show_col_types = FALSE\n)\n\nweekly_gas_prices |&gt;\n  ggplot(aes(x = date, y = price)) +\n  geom_line(color = \"steelblue\") +\n  labs(\n    title = \"Weekly U.S. Gas Prices (1990–2025)\",\n    x = \"Date\",\n    y = \"Price (USD per gallon)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nOver the past 30 years, U.S. gas prices have shown strong sensitivity to global economic events and policy shifts. Prices rose sharply during the early 2000s while their was growing demand and geopolitical instability, peaking before the 2008 financial crisis. After the ’08 crisis, prices fluctuated through the 2010’s due to oil production levals and OPEC decisions. More recently, pandemic-related supply shocks and the 2022 energy crisis drove another major surge, followed by gradual stabilization.\nimage source: CNBC"
  }
]