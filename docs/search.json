[
  {
    "objectID": "Ethics_Blog.html",
    "href": "Ethics_Blog.html",
    "title": "An Analyisis of Data Ethics",
    "section": "",
    "text": "Considerations:\n\nWhat is the permission structure for using the data? Was it followed?\n\nFrom the Guardian Article, we learn that the permission structure claimed to be “aceademic research” for the users who consented to participate in the personality test. I compared the following information with my own research of Facebook’s permissions structure. According to the Guardian, Facebook’s privacy policy did allow researchers to collect not only direct user data, but also that of friends (Wong, Guardian). Facebook does confirm this, in both court hearings and through legal documentation I was able to find. However, because friends did not explicitly give permission, this permission structure was compromised. The NYT article builds on this, adding that Facebook’s own policy required that apps/tests ask for “a clear privacy policy and user authorization” and that Facebook has the ability to revoke data access when misuse is detected, but FB did not sufficiently alert impacted users. Thus, although a formal permission structure existed (Facebook’s platform rules, app disclosure), it was not followed in practice by the parties involved.\n\nWhat was the consent for recruiting participants?\n\nAccording to the Guardian, only a few thousand users who ended up installing the personality test app knowingly supplied their data under the pretense of “academic research”. None of their FB friends were asked directly for consent. From the NYT article, we get the picture that Facebook’s terms required “informed consent” for data usage, yet the app’s true purpose (political targeting) was not disclosed. Thus, the consent given was incomplete: it was partially informed and did not cover the large network of friends whose data was used.\n\nIs the data being used in unintended ways?\n\nBoth articles make the push that the data was clearly being used in dishonest ways. The Guardian piece goes into detail on how Cambridge Analytica was able to create psychographic modeling by turning likes and friend networks into political persuasion mechanisms that could be weaponized by political parties. Researchers could infer these traits using the OCEAN model (Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism), which allowed them to predict how individuals might respond to different political messages. The company combined this with network analysis, exploiting friend connections and engagement metadata to build a comprehensive psychological map of the electorate. The New York Times emphasizes that Facebook’s algorithmic infrastructure facilitated this misuse by allowing developers to collect not only direct users’ data but also their entire social networks, creating a web of data that could be re-purposed for political manipulation. Facebook’s platform rules clearly dictate that data must be used only for the purpose that was disclosed to users. In this case, that clear requirement was not adhered to.\n\nWho was being measured?\n\nBoth articles go into depth about the models that were being used to track users. Given quiz results, location, behaviors, and how personality trends typically are similar to those of your friends. Through network inference, the company could identify clusters of users with similar traits and estimate political tendencies even for those who had never interacted with the quiz. The data science here exploited network data and behavioral metadata to profile people.\nSummary: Why does this matter?\nThis case is relevant because it exists at the intersection of fundamental issues of autonomy, democracy, and privacy. Those who benefited included data brokers, campaign strategists, and platform owners who were able to use the data to influence and profit. Those harmed were ordinary users whose data was collected without full knowledge, but almost as significant, democratic processes that were manipulated by opaque micro-targeting.\nSince the break of the scandal, Facebook and Cambridge Analytica have faced significant legal battles. As a result, Facebook said it removed the app in 2015 and required certification from everyone, with copies stating that the data had been destroyed. In a statement by Paul Grewal, Facebook’s vice-president, he stated, “We are committed to vigorously enforcing our policies to protect people’s information. We will take whatever steps are required to see that this happens,” (Confessore, NYT). The ethical violations appear to be driven by profit rather than transparency or public good. In a world where our data is so easily accessible, this scenario is a cautionary exemplar of why stronger ethics, governance, and user consent are vital.\nimage source: The Guardian\nSources Cited:\n[Wong, Julia Carrie. “The Cambridge Analytica Scandal Changed the World – but It Didn’t Change Facebook.” The Guardian, The Guardian, 18 Mar. 2019] (www.theguardian.com/technology/2019/mar/17/the-cambridge-analytica-scandal-changed-the-world-but-it-didnt-change-facebook)\n[Confessore, Nicholas. “Cambridge Analytica and Facebook: The Scandal and the Fallout so Far.” Nytimes.com, The New York Times, 4 Apr. 2018,] (www.nytimes.com/2018/04/04/us/politics/cambridge-analytica-scandal-fallout.html)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thomas Price",
    "section": "",
    "text": "Hi! My name is Thomas Price and I am a current Senior at Pitzer College. At Pitzer, I study International Political Economy and Data Science. When I am not studying, you can find me playing tennis on our club tennis team, traveling around the country with the Claremont Debate Team or winning intramural Pickle ball Tournaments!\n\nBeing From San Francisco, I am a huge Giants fan, so if you support any LA team, kindly leave this website"
  },
  {
    "objectID": "simulation.html",
    "href": "simulation.html",
    "title": "Understanding the Relationship Between GDP Per-Capita and Life Expectancy",
    "section": "",
    "text": "image source: Getty Images\n\n\nCode\n## Selecting data to be used\nlibrary(gapminder)\nlibrary(tidyverse)\nhead(gapminder)\n\n\n# A tibble: 6 × 6\n  country     continent  year lifeExp      pop gdpPercap\n  &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 Afghanistan Asia       1952    28.8  8425333      779.\n2 Afghanistan Asia       1957    30.3  9240934      821.\n3 Afghanistan Asia       1962    32.0 10267083      853.\n4 Afghanistan Asia       1967    34.0 11537966      836.\n5 Afghanistan Asia       1972    36.1 13079460      740.\n6 Afghanistan Asia       1977    38.4 14880372      786.\n\n\nCode\ngap2007 &lt;- gapminder |&gt;\n  filter(year == 2007) |&gt;\n  select(country, continent, lifeExp, gdpPercap)\n\n\n\n\nCode\nggplot(gap2007, aes(x = lifeExp, y = gdpPercap, color = continent)) +\n  geom_point(size = 2, alpha = 0.8) +\n  labs(\n    title = \"Life Expectancy vs GDP per Capita (2007)\",\n    subtitle = \"Each point represents a country\",\n    x = \"Life Expectancy (years)\",\n    y = \"GDP per Capita (US dollars)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\nggplot(gap2007, aes(x = lifeExp, y = gdpPercap, color = continent)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE) + \n  labs(\n    title = \"Life Expectancy vs GDP per Capita (2007)\",\n    subtitle = \"Each point represents a country\",\n    x = \"Life Expectancy (years)\",\n    y = \"GDP per Capita (US dollars)\"\n  ) +\n  facet_wrap(~continent) + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe scatter plot shows the observed relationship between life expectancy and GDP per capita across countries in 2007. Their is a general upward trend where countries with longer life expectancy tend to have higher incomes. Additionally, their is noticeable variation within continents. Some countries achieve high life expectancy despite modest GDP per capita, and others show the opposite.This graph warrants testing whether the relationship is strong enough to be considered statistically meaningful.\nThis faceted graph shows the relationship between life expectancy and GDP per capita in 2007, separated by continent. Each point represents a country, and the line within each panel indicates the general direction of the relationship. Overall, continents like Europe and Asia show a strong positive correlation, while Africa and Oceania display weaker associations between life expectancy and GDP per capita.\n\n\nCode\n#Identifying the LifeExp slope with a linear model\n\nget_slope &lt;- function(df) {\n  fit &lt;- lm(gdpPercap ~ lifeExp, data = df)\n  coef(fit)[[\"lifeExp\"]]  \n}\n\n\nThe code above tests the slope of the simple regression. gdpPercap ~ lifeExp\nNull : beta1 = 0 Alt : beta1 ≠ 0\n\n\nCode\nobs_slopes &lt;- gap2007 |&gt;\n  group_by(continent) |&gt;\n  summarize(\n    obs_slope = coef(lm(gdpPercap ~ lifeExp))[2]\n  )\n\nobs_slopes\n\n\n# A tibble: 5 × 2\n  continent obs_slope\n  &lt;fct&gt;         &lt;dbl&gt;\n1 Africa         145.\n2 Americas      1293.\n3 Asia          1225.\n4 Europe        3366.\n5 Oceania       8972.\n\n\nCode\ncontinents &lt;- unique(gap2007$continent)\n\nobs_slopes_continent &lt;- numeric(length(continents))\n\nfor (i in 1:length(continents)) {\n  df_cont &lt;- subset(gap2007, continent == continents[i])\n  \n  obs_slopes_continent[i] &lt;- get_slope(df_cont)\n}\n\nnames(obs_slopes_continent) &lt;- continents\n\nobs_slope_overall &lt;- get_slope(gap2007)\nobs_slope_overall\n\n\n[1] 722.8975\n\n\nCode\nobs_slopes_continent\n\n\n     Asia    Europe    Africa  Americas   Oceania \n1225.2852 3365.9658  144.5327 1292.5570 8972.2195 \n\n\nThe above code uses a for loop to compute the observed slope for each continent. For each iteration, the code filtered the data set to a single continent, ran the get_slope() function and saved the slope value. This allows me to compare how the strength of the relationship between life expectancy and GDP per capita differs geographically.Given the slope output, we can predict the average change in GDP per capita (US$) for a 1 year increase in life. For example in Asia, we can predict 1 additional year of life yields 1225.2 additional gdp per capita.\nLarger positive slopes signify a steeper income to health relationship.\n\n\nCode\n# Permutation Test : Null Distribution\nperm_null &lt;- function(rep, data){\n  data |&gt;\n    group_by(continent) |&gt;\n    select(continent, lifeExp, gdpPercap) |&gt;\n    mutate(lifeExp_perm = sample(lifeExp, replace = FALSE)) |&gt;\n    summarize(\n      obs_slope  = coef(lm(gdpPercap ~ lifeExp      ))[2],\n      perm_slope = coef(lm(gdpPercap ~ lifeExp_perm ))[2],\n      .groups = \"drop\"\n    ) |&gt;\n    mutate(rep = rep)\n}\n\nset.seed(2025)\nB &lt;- 2000\n\n\nnull_by_cont_tbl &lt;- map(1:B, perm_null, data = gap2007) |&gt;\n  list_rbind()\n\nhead(null_by_cont_tbl)\n\n\n# A tibble: 6 × 4\n  continent obs_slope perm_slope   rep\n  &lt;fct&gt;         &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1 Africa         145.      -18.0     1\n2 Americas      1293.     -205.      1\n3 Asia          1225.     -155.      1\n4 Europe        3366.     -514.      1\n5 Oceania       8972.    -8972.      1\n6 Africa         145.       22.6     2\n\n\nTo simulate the null hypothesis that life expectancy and GDP per capita are unrelated, I created a permutation function. The function (perm_null) randomly shuffles life expectancy values within the dataset. For each randomization, a linear model is fit for each continent and the new slope is recorded. Repeating this 2,000 times using the map() function creates a null distribution of slopes that would occur purely by chance.\n\n\nCode\n#Calculating P-Values per continent \n\npvals_tbl &lt;- null_by_cont_tbl |&gt;\n  group_by(continent) |&gt;\n  summarize(\n    #obs_slope = first(obs_slope), \n    #p_value = mean(abs(perm_slope) &gt;= abs(obs_slope))\n    p_value = mean(perm_slope &gt;= obs_slope)\n  )\n\npvals_tbl\n\n\n# A tibble: 5 × 2\n  continent p_value\n  &lt;fct&gt;       &lt;dbl&gt;\n1 Africa      0.004\n2 Americas    0    \n3 Asia        0    \n4 Europe      0    \n5 Oceania     0.482\n\n\nThe permutation results show that the relationship between life expectancy and GDP per capita is statistically significant in Africa, Asia, the Americas, and Europe. The effect in Africa is far smaller which may indicate that economic development does not translate into health outcomes as consistently there as it does in other parts of the world. Oceania shows a non-significant results because only two countries are present, making any estimate unstable. Overall, the analysis suggests that while income and health are positively related worldwide, the strength of this relationship varies by region.\n\n\nCode\n#Plotting -- Faceted null distribution plots \n\nobs_slopes &lt;- gap2007 |&gt;\n  group_by(continent) |&gt;\n  summarize(obs_slope = coef(lm(gdpPercap ~ lifeExp))[2], .groups = \"drop\")\n\nggplot() +\n  geom_histogram(\n    data = null_by_cont_tbl,\n    aes(x = perm_slope),\n    bins = 40, fill = \"lightblue\", color = \"white\"\n  ) +  \n  geom_vline(\n    data = obs_slopes,\n    aes(xintercept = obs_slope),\n    color = \"red\", linewidth = .8\n  ) +\n  facet_wrap(~ continent, scales = \"free\") +\n  labs(\n    title = \"Permutation Test: Null Distribution of Slopes by Continent\",\n    subtitle = \"Red line = observed slope; histogram = null slopes (lifeExp shuffled)\",\n    x = \"Permuted slopes\", y = \"Count\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe histograms show slopes expected by chance if life expectancy and GDP were unrelated, while the red lines show the true slopes. In Asia, Europe, and the Americas, the observed slopes fall far outside the null distributions, indicating a strong real relationship. Africa shows a weaker but still significant effect. Oceania does not show significance because it includes only two countries, making its estimate unreliable.\nConclusion:\nThis simulation study provides strong evidence of a positive relationship between life expectancy and GDP per capita worldwide. However, this strength of correlation is dependent on region. Europe, Asia and the Americas show a strong and significant relationship. Africa shows a weaker, yet still significant connection. Oceania’s results are not meaningfull due to having only two countries in the dataset. Overall, economic development and improved health outcomes appear linked globally, but the degree of the link is highly dependent on regional context."
  },
  {
    "objectID": "billboard_t100.html",
    "href": "billboard_t100.html",
    "title": "Billboard Hot 100 Number Ones",
    "section": "",
    "text": "View Original Data Set Here\n\n\nCode\nlibrary(tidyverse)\n\nbillboard &lt;- readr::read_csv(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-08-26/billboard.csv\"\n)\n\nartist_counts &lt;- billboard |&gt;\n  group_by(artist) |&gt;\n  summarise(n = n()) |&gt;\n  arrange(desc(n))\n\ntop10_artists &lt;- head(artist_counts, 10)\n\n\nggplot(top10_artists, aes(x = reorder(artist, n), y = n)) +\n  geom_col(fill = \"steelblue\") +\n  geom_text(aes(label = n), hjust = 1.2, color = \"white\", size = 4) +\n  coord_flip() +   # flips so artists are readable\n  labs(\n    title = \"Top 10 Artists by Number of Hot 100 Songs\",\n    x = \"Artist\",\n    y = \"Number of Songs\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nimage source: Wikipedia"
  },
  {
    "objectID": "Proj2.html",
    "href": "Proj2.html",
    "title": "Analysis of Obama’s Tweets",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(lubridate)\nlibrary(ggrepel)\n\n# Reading the data\nobama &lt;- read_csv(\"Obama_Tweets/tweets.csv\")\n\n# Cleaning data\ntweets &lt;- obama |&gt;\n  filter(!is.na(text)) |&gt; \n  mutate(\n    text_lower = str_to_lower(text),\n    text_noline = str_replace_all(text_lower, \"[\\r\\n]\", \" \")\n  )\n\nhead(tweets)\n\n\n# A tibble: 6 × 12\n  tweet_id in_reply_to_status_id in_reply_to_user_id timestamp      source text \n     &lt;dbl&gt;                 &lt;dbl&gt;               &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;  &lt;chr&gt;\n1  7.97e17                    NA                  NA 2016-11-11 17… \"&lt;a h… Toda…\n2  7.96e17                    NA                  NA 2016-11-08 15… \"&lt;a h… Toda…\n3  7.94e17                    NA                  NA 2016-11-03 23… \"&lt;a h… It's…\n4  7.94e17                    NA                  NA 2016-11-03 06… \"&lt;a h… It h…\n5  7.90e17                    NA                  NA 2016-10-23 17… \"&lt;a h… I'll…\n6  7.90e17                    NA                  NA 2016-10-21 22… \"&lt;a h… Chec…\n# ℹ 6 more variables: retweeted_status_id &lt;dbl&gt;,\n#   retweeted_status_user_id &lt;dbl&gt;, retweeted_status_timestamp &lt;chr&gt;,\n#   expanded_urls &lt;chr&gt;, text_lower &lt;chr&gt;, text_noline &lt;chr&gt;\n\n\nThe above graph is the data taken from the National Archives of Obama’s tweets. He had a total of 325 tweets through is 2 year Twitter tenure. Moving forward, we will continue to clean the data to identify what the nature of his tweets where, specifically targeting those that had links in them. This specific table shows, the date of the tweet, the source, the text of the tweet, if it was re-tweeted, and any urls within the tweet. The cleaning function we did removes any rows where the tweet text is missing, and does some basic cleaning by making all the text lowercase and getting rid of any line breaks.\n\n\nCode\n# Identifying tweets that contain links and government links. \n\n#| warning: false\n#| message: false\n\n\nlibrary(tidyverse)\nlibrary(stringr)\n\ntweets_table &lt;- tweets |&gt;\n  mutate(\n    created_at = timestamp,\n    has_link = !is.na(expanded_urls),\n\n    gov_link = str_detect(expanded_urls, \"(?&lt;=https?://)[^\\\\s]*\\\\.gov\"), has_http_in_text = str_detect(text, \"https?://[^\\\\s]+\")) |&gt;\n     select(created_at, text, has_link, gov_link, has_http_in_text)\n\n\n\n\nCode\n#Creating a new table with those links \n#| warning: false\n#| message: false\n\n\ntweets_table |&gt;\n  summarise(\n    total_tweets = n(),\n    with_links = sum(has_link),\n    pct_with_links = mean(has_link),\n    pct_gov_of_links = mean(gov_link[has_link], na.rm = TRUE)\n  )\n\n\n# A tibble: 1 × 4\n  total_tweets with_links pct_with_links pct_gov_of_links\n         &lt;int&gt;      &lt;int&gt;          &lt;dbl&gt;            &lt;dbl&gt;\n1          321        135          0.421            0.126\n\n\nThis table showcases the total number of tweets that Obama posted during his short Twitter tenure. The table highlights tweets that contained links, and further identifies the percentage of those links that directed users to government websites. We can see that there were a total of 321 tweets, 135 of which contained links (42%). Among those tweets, 12.5% included links to government websites.\n\n\nCode\n#Visualizing which tweets contained links vs Gov links\n#| warning: false\n#| message: false\n\n\nlibrary(lubridate)\nlibrary(ggplot2)\n\nlinks_summary &lt;- tweets_table |&gt;\n  summarise(\n    with_links = sum(has_link, na.rm = TRUE),\n    with_gov_links = sum(gov_link, na.rm = TRUE)\n  ) |&gt;\n  pivot_longer(cols = everything(), names_to = \"type\", values_to = \"count\")\n\nggplot(links_summary, aes(x = type, y = count, fill = type)) +\n  geom_col(width = 0.5) +\n  labs(\n    title = \"Obama Tweets Containing .Gov and Non .Gov Links\",\n    x = NULL,\n    y = \"Number of Tweets\"\n  ) +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"steelblue\", \"darkred\"))\n\n\n\n\n\n\n\n\n\nThe graph above showcases that table data above. We can see the total amount of tweets that contained links, and the total amount that contained government links. As seen only a small percentage where gov links. Given that Twitter is a relatively new medium to disseminate critical information, I would like to do further analysis to see if more recent officials have used it to push government resources. Ultimately comparing usage across different administrations."
  },
  {
    "objectID": "Proj5.html",
    "href": "Proj5.html",
    "title": "Racial Disparities in Police Search Practices Across Three California Cities",
    "section": "",
    "text": "View Original Data Set Here\nReference: A large-scale analysis of racial disparities in police stops across the United States, Nature Human Behaviour\nOriginal Author: Pierson, E., Simoiu, C., Overgoor, J., Corbett-Davies, S., Ramachandran, V., Phillips, C., Shroff, R., & Goel, S. (2020).\n\n\nCode\nlibrary(DBI)\nlibrary(RMariaDB)\n\ncon_traffic &lt;- dbConnect(\n  RMariaDB::MariaDB(),\n  dbname = \"traffic\",\n  host = Sys.getenv(\"TRAFFIC_HOST\"),\n  user = Sys.getenv(\"TRAFFIC_USER\"),\n  password = Sys.getenv(\"TRAFFIC_PWD\")\n)\n\n\nViewing SF lables\n\n\nCode\nSHOW COLUMNS FROM ca_san_francisco_2020_04_01;\n\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nraw_row_number\ntext\nYES\n\nNA\n\n\n\ndate\ndate\nYES\n\nNA\n\n\n\ntime\ntime\nYES\n\nNA\n\n\n\nlocation\ntext\nYES\n\nNA\n\n\n\nlat\ndouble\nYES\n\nNA\n\n\n\nlng\ndouble\nYES\n\nNA\n\n\n\ndistrict\ntext\nYES\n\nNA\n\n\n\nsubject_age\nbigint(20)\nYES\n\nNA\n\n\n\nsubject_race\ntext\nYES\n\nNA\n\n\n\nsubject_sex\ntext\nYES\n\nNA\n\n\n\n\n\n\nViewing LA labels\n\n\nCode\nSHOW COLUMNS FROM ca_los_angeles_2020_04_01;\n\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nraw_row_number\ntext\nYES\n\nNA\n\n\n\ndate\ndate\nYES\n\nNA\n\n\n\ntime\ntime\nYES\n\nNA\n\n\n\ndistrict\ntext\nYES\n\nNA\n\n\n\nregion\ntext\nYES\n\nNA\n\n\n\nsubject_race\ntext\nYES\n\nNA\n\n\n\nsubject_sex\ntext\nYES\n\nNA\n\n\n\nofficer_id_hash\ntext\nYES\n\nNA\n\n\n\ntype\ntext\nYES\n\nNA\n\n\n\nraw_descent_description\ntext\nYES\n\nNA\n\n\n\n\n\n\nViewing San Diego Lables\n\n\nCode\nSHOW COLUMNS FROM ca_san_diego_2020_04_01;\n\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nraw_row_number\ntext\nYES\n\nNA\n\n\n\ndate\ndate\nYES\n\nNA\n\n\n\ntime\ntime\nYES\n\nNA\n\n\n\nservice_area\ntext\nYES\n\nNA\n\n\n\nsubject_age\nbigint(20)\nYES\n\nNA\n\n\n\nsubject_race\ntext\nYES\n\nNA\n\n\n\nsubject_sex\ntext\nYES\n\nNA\n\n\n\ntype\ntext\nYES\n\nNA\n\n\n\narrest_made\ndouble\nYES\n\nNA\n\n\n\ncitation_issued\ndouble\nYES\n\nNA\n\n\n\n\n\n\nViewing San Jose\n\n\nCode\nDESCRIBE ca_san_jose_2020_04_01;\n\n\n\nDisplaying records 1 - 10\n\n\nField\nType\nNull\nKey\nDefault\nExtra\n\n\n\n\nraw_row_number\ntext\nYES\n\nNA\n\n\n\ndate\ndate\nYES\n\nNA\n\n\n\ntime\ntime\nYES\n\nNA\n\n\n\nlocation\ntext\nYES\n\nNA\n\n\n\nlat\ndouble\nYES\n\nNA\n\n\n\nlng\ndouble\nYES\n\nNA\n\n\n\nsubject_race\ntext\nYES\n\nNA\n\n\n\ntype\ntext\nYES\n\nNA\n\n\n\narrest_made\ndouble\nYES\n\nNA\n\n\n\ncitation_issued\ndouble\nYES\n\nNA\n\n\n\n\n\n\nViewing SF, SD, and SJ search & hit rates by race\n\n\nCode\nSELECT \n    'San Francisco' AS city,\n    subject_race,\n    COUNT(*) AS n_stops,\n    AVG(search_conducted = 1) AS search_rate,\n    AVG(CASE WHEN search_conducted = 1 THEN contraband_found = 1 END) AS hit_rate\nFROM ca_san_francisco_2020_04_01\nWHERE subject_race IS NOT NULL\nGROUP BY subject_race\n\nUNION ALL\n\nSELECT \n    'San Diego' AS city,\n    subject_race,\n    COUNT(*) AS n_stops,\n    AVG(search_conducted = 1) AS search_rate,\n    AVG(CASE WHEN search_conducted = 1 THEN contraband_found = 1 END) AS hit_rate\nFROM ca_san_diego_2020_04_01\nWHERE subject_race IS NOT NULL\nGROUP BY subject_race\n\nUNION ALL\n\nSELECT \n    'San Jose' AS city,\n    subject_race,\n    COUNT(*) AS n_stops,\n    AVG(search_conducted = 1) AS search_rate,\n    AVG(CASE WHEN search_conducted = 1 THEN contraband_found = 1 END) AS hit_rate\nFROM ca_san_jose_2020_04_01\nWHERE subject_race IS NOT NULL\nGROUP BY subject_race\n\nORDER BY city, subject_race;\n\n\n\n\nCode\nrates\n\n\n            city           subject_race n_stops search_rate hit_rate\n1      San Diego asian/pacific islander   32541      0.0280   0.1044\n2      San Diego                  black   42705      0.0907   0.0932\n3      San Diego               hispanic  117083      0.0555   0.0812\n4      San Diego                  other   27238      0.0166   0.0953\n5      San Diego                  white  162226      0.0278   0.1149\n6  San Francisco asian/pacific islander  157684      0.0181   0.3611\n7  San Francisco                  black  152196      0.1552   0.0924\n8  San Francisco               hispanic  116014      0.0987   0.1018\n9  San Francisco                  other  106858      0.0351   0.2032\n10 San Francisco                  white  372318      0.0314   0.2421\n11      San Jose asian/pacific islander   16062      0.1352   0.1800\n12      San Jose                  black   13538      0.3280   0.1867\n13      San Jose               hispanic   79885      0.3420   0.1571\n14      San Jose                  other   11523      0.1768   0.2274\n15      San Jose                  white   26341      0.2454   0.1984\n\n\nCleaning the data in order to visualize\n\n\nCode\nlibrary(tidyverse)\n\nrates_clean &lt;- rates |&gt;\n  mutate(\n    search_rate = round(search_rate * 100, 1),\n    hit_rate    = round(hit_rate * 100, 1)\n  )\n\n\nVisualization 1: Search Rates by Race\n\n\nCode\nggplot(rates_clean, aes(x = subject_race, y = search_rate, fill = city)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Search Rates by Race Across Three California Cities\",\n    x = \"Race\",\n    y = \"Search Rate (%)\",\n    fill = \"City\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nThe figure above compares the likelihood that officers searched drivers from different racial groups across the three cities. What is clear is that the search rate varies significantly by race. In all three cities, Latino and Black drivers were searched at significantly higher rates than White and Asian drivers. These results help suggest that the decision to stop and search drivers is not uniformly applied across racial groups. Furthermore, the data seems skewed because San Jose has higher hit rates across all racial groups. This does not mean that individuals in San Jose are more likely to merit searches; instead, it reflects systematic differences in policing practices and reporting standards. San Jose likely documents searches more frequently and consistently or, more plausibly, operates under a system that has policies to increase the number of searches per stop.\nVisualization 2: Hit rates of contraband found by race\n\n\nCode\nggplot(rates_clean, aes(x = subject_race, y = hit_rate, fill = city)) +\n  geom_col(position = \"dodge\") +\n  labs(\n    title = \"Contraband Hit Rates by Race Across Three California Cities\",\n    x = \"Race\",\n    y = \"Hit Rate (%)\",\n    fill = \"City\"\n  ) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nThe contribution hit rate seen above shows significant differences in search effectiveness across the three cities. San Francisco consistently has the highest hit rates across racial groups, which suggests that its officers conduct fewer but far more targeted searches. San Jose conducts far more searches but has only mid-level hit rates, indicating a lower threshold for initiating searches that frequently do not result in contraband. San Diego has the lowest hit rates overall, which implies that many searches in the city are not based on strong suspicion. As a takeaway, it seems that San Francisco’s searches are far more effective, San Jose’s are more frequent but less productive, and San Diego’s are both infrequent and largely unsuccessful. Overall, this plot highlights how policing strategies are highly varied and city-dependent, even when they all operate in the same state.\nVisualization 3\n\n\nCode\nlibrary(ggplot2)\nlibrary(dplyr)\n\nrates_eff &lt;- rates |&gt;\n  mutate(\n    search_rate_pct = search_rate * 100,\n    hit_rate_pct = hit_rate * 100\n  )\n\nggplot(rates_eff, aes(x = search_rate_pct, \n                      y = hit_rate_pct, \n                      color = city, \n                      size = n_stops, \n                      label = subject_race)) +\n  geom_point(alpha = 0.8) +\n  geom_text(vjust = -0.8, size = 3, show.legend = FALSE) +\n  labs(\n    title = \"Search Efficiency: Search Rates vs Hit Rates Across Racial Groups\",\n    subtitle = \"Each point sized by number of stops; labels represent racial groups\",\n    x = \"Search Rate (%)\",\n    y = \"Hit Rate (%)\",\n    color = \"City\",\n    size = \"Number of Stops\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size = 14, face = \"bold\"))\n\n\n\n\n\n\n\n\n\nFor this visualization, I was interested in exploring whether some groups are searched more but found with contraband less often. On the contrary, there are some groups that are searched less but found with contraband more. The goal was to help unpack biases that officers may have. The plot compares how different racial groups are searched and how often those searches actually find contributors. San Diego shows both low search rates and low hit rates across all racial groups, which suggests fewer searches overall but simultaneously less efficiency. San Francisco, by contrast, shows moderate search rates paired with higher hit rates, which indicates more targeted, successful searches. San Jose consistently has the highest search rates but only mid-level hit rates. This implies that officers search more often but are way less efficient.\nConclusion-\nAcross San Francisco, San Diego and San Jose, the data above highlights clear racial disparities when it comes to the frequency of drivers being searched and how effective those searches are. Cities have stark differences in regards to search intensity as well as efficiency. San Francisco for example, conducts fewer yet more targeted searches. Where San Jose searches far more often but with lower sucess rates.\nTogather, these results help suggest that policing stratagies and potential biases vary substantially across police jurisdictions. This helps underscore the need for consistant statewide oversight that involves data driven policy reform.\nSources Cited:\nPierson, Emma, et al. “A Large-Scale Analysis of Racial Disparities in Police Stops Across the United States.” Nature Human Behaviour, 2020. Data from: Stanford Open Policing Project (https://openpolicing.stanford.edu ).\nimage source: OpenPolicing"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Thomas Price, a junior at Pitzer College majoring in International Political Economy and minoring in Data Science. I recently studied abroad in Spain, and the photo above is from a trip my friends and I took to Monte Carlo! The purpose of this website is to showcase how one can wrangle and visulualize data, feel free to find those examples in the drop down menu on your left!"
  },
  {
    "objectID": "about.html#bio",
    "href": "about.html#bio",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Thomas Price, a junior at Pitzer College majoring in International Political Economy and minoring in Data Science. I recently studied abroad in Spain, and the photo above is from a trip my friends and I took to Monte Carlo! The purpose of this website is to showcase how one can wrangle and visulualize data, feel free to find those examples in the drop down menu on your left!"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\n\nSummer Analyst at AlphaSights (San Francisco)\n\nStrategy Intern at BCIU (Washington,DC)\nLegislative Intern at the US House of Repersentitives (Washington, DC)"
  },
  {
    "objectID": "about.html#projects",
    "href": "about.html#projects",
    "title": "About",
    "section": "Projects",
    "text": "Projects\n\nSenior thesis: Marshall Plan vs. Belt & Road Initiative\n\nData wrangling/visualization with R + ggplot"
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About",
    "section": "Contact",
    "text": "Contact\nFeel free to connect with me: - LinkedIn\n- GitHub\n- Email —"
  },
  {
    "objectID": "tidygas.html",
    "href": "tidygas.html",
    "title": "US Gas Prices",
    "section": "",
    "text": "View the original TidyTuesday dataset\n\n\nCode\nlibrary(tidyverse)\n\nweekly_gas_prices &lt;- readr::read_csv(\n  \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-07-01/weekly_gas_prices.csv\",\n  show_col_types = FALSE\n)\n\nweekly_gas_prices |&gt;\n  ggplot(aes(x = date, y = price)) +\n  geom_line(color = \"steelblue\") +\n  labs(\n    title = \"Weekly U.S. Gas Prices (1990–2025)\",\n    x = \"Date\",\n    y = \"Price (USD per gallon)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nimage source: CNBC"
  }
]