---
title: "An Analyisis of Data Ethics"

subtitle: "Analyzing the Cambridge Analytica and Facebook Data Breach" 
description: |
 In this blog entry, I focus on a unique ethical dilemma that occurred in 2016, when research journalists from the Guardian uncovered a severe data ethics violation. A consulting company named Cambridge Analytica had harvested user data from more than 50 million Facebook user profiles. According to the Guardian, Cambridge Analytica gathered this data through paying a few thousand users of Facebook to take a personality test, through an app, in which they agreed to have their data collected for academic use. What they were not aware of was that this test also collected information about the test-takers’ Facebook friends, leading to the accumulation of a large data pool. This data was then used to build psychological profiles and models that would be sold to political campaigns. In a subsequent New York Times article, the story is expanded: Facebook discovered the misuse, failed to properly alert users, and the data was used to deliver targeted political advertisements in the 2016 U.S. election and other campaigns. (New York Times, April 2018). The data-science component lies in the use of big user data (friend networks, demographics, likes) plus machine‐learning / psychometric modelling to segment users and deliver tailored political messaging. 

  The major ethical dilemma at play here is: Can data, collected under one pretence, be used or repurposed for an entirely different purpose? In this case, it was used for political influence without informed consent. On two levels, how does this affect personal autonomy and democracy as a whole? 
 
 Refrence: [Article 1: The Guardian](https://www.theguardian.com/technology/2019/mar/17/the-cambridge-analytica-scandal-changed-the-world-but-it-didnt-change-facebook)
         | [Article 2: The New York Times](https://www.nytimes.com/2018/04/04/us/politics/cambridge-analytica-scandal-fallout.html)

  Authors: Julia Wong (Guardian), Nicholas Confessore (NYT)
format:
  html:
    css: style.css
    code-fold: true
about:
 template: jolla
 image: images/Zuck.jpeg 
 image-shape: rounded
 image-width: 25em
---

Considerations: 

1. What is the permission structure for using the data? Was it followed?

From the Guardian Article, we learn that the permission structure claimed to be “aceademic research” for the users who consented to participate in the personality test. I compared the following information with my own research of Facebook's permissions structure. According to the Guardian, Facebook’s privacy policy did allow researchers to collect not only direct user data, but also that of friends (Wong, Guardian). Facebook does confirm this, in both court hearings and through legal documentation I was able to find. However, because friends did not explicitly give permission, this permission structure was compromised. The NYT article builds on this, adding that Facebook’s own policy required that apps/tests ask for “a clear privacy policy and user authorization” and that Facebook has the ability to revoke data access when misuse is detected, but FB did not sufficiently alert impacted users. Thus, although a formal permission structure existed (Facebook’s platform rules, app disclosure), it was not followed in practice by the parties involved.


2. What was the consent for recruiting participants?

According to the Guardian, only a few thousand users who ended up installing the personality test app knowingly supplied their data under the pretense of “academic research”. None of their FB friends were asked directly for consent. From the NYT article, we get the picture that Facebook’s terms required “informed consent” for data usage, yet the app’s true purpose (political targeting) was not disclosed. Thus, the consent given was incomplete: it was partially informed and did not cover the large network of friends whose data was used.


3. Is the data being used in unintended ways?

Both articles make the push that the data was clearly being used in dishonest ways. The Guardian piece goes into detail on how Cambridge Analytica was able to create psychographic modeling by turning likes and friend networks into political persuasion mechanisms that could be weaponized by political parties. Researchers could infer these traits using the OCEAN model (Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism), which allowed them to predict how individuals might respond to different political messages. The company combined this with network analysis, exploiting friend connections and engagement metadata to build a comprehensive psychological map of the electorate. The New York Times emphasizes that Facebook’s algorithmic infrastructure facilitated this misuse by allowing developers to collect not only direct users’ data but also their entire social networks, creating a web of data that could be re-purposed for political manipulation. Facebook's platform rules clearly dictate that data must be used only for the purpose that was disclosed to users. In this case, that clear requirement was not adhered to. 


4. Who was being measured? 

Both articles go into depth about the models that were being used to track users. Given quiz results, location, behaviors, and how personality trends typically are similar to those of your friends. Through network inference, the company could identify clusters of users with similar traits and estimate political tendencies even for those who had never interacted with the quiz. The data science here exploited network data and behavioral metadata to profile people. 


Summary: Why does this matter?

This case is relevant because it exists at the intersection of fundamental issues of autonomy, democracy, and privacy. Those who benefited included data brokers, campaign strategists, and platform owners who were able to use the data to influence and profit. Those harmed were ordinary users whose data was collected without full knowledge, but almost as significant, democratic processes that were manipulated by opaque micro-targeting.

Since the break of the scandal, Facebook and Cambridge Analytica have faced significant legal battles. As a result, Facebook said it removed the app in 2015 and required certification from everyone, with copies stating that the data had been destroyed. In a statement by Paul Grewal, Facebook’s vice-president, he stated, “We are committed to vigorously enforcing our policies to protect people’s information. We will take whatever steps are required to see that this happens,” (Confessore, NYT). The ethical violations appear to be driven by profit rather than transparency or public good. In a world where our data is so easily accessible, this scenario is a cautionary exemplar of why stronger ethics, governance, and user consent are vital. 





[image source: The Guardian](https://www.theguardian.com/technology/2019/mar/17/the-cambridge-analytica-scandal-changed-the-world-but-it-didnt-change-facebook)

Sources Cited: 

  [Wong, Julia Carrie. “The Cambridge Analytica Scandal Changed the World – but It Didn’t Change Facebook.” The Guardian, The Guardian, 18 Mar. 2019] (www.theguardian.com/technology/2019/mar/17/the-cambridge-analytica-scandal-changed-the-world-but-it-didnt-change-facebook)



  [Confessore, Nicholas. “Cambridge Analytica and Facebook: The Scandal and the Fallout so Far.” Nytimes.com, The New York Times, 4 Apr. 2018,] (www.nytimes.com/2018/04/04/us/politics/cambridge-analytica-scandal-fallout.html)

