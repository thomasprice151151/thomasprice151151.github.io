---
title: "An Analyisis of Data Ethics"

subtitle: "Analyzing the Cambridge Analytica and Facebook Data Breach" 
description: |
 In this blog entry, I examine a major ethical dilemma that emerged in 2016 when investigative journalists from The Guardian uncovered a serious breach of data ethics involving the political consulting firm Cambridge Analytica. “The company had harvested personal data from more than 50 million Facebook user profiles without proper consent” (Wong, 2018). Cambridge Analytica acquired this data by paying a few thousand Facebook users to complete a personality test through an app that claimed to collect information for academic research.

  As The New York Times reported, the app, titled “This Is Your Digital Life”, was created by Aleksandr Kogan, a Cambridge University researcher who also operated a private firm called Global Science Research (GSR). Cambridge Analytica contracted GSR to design and distribute this app, which covertly harvested not only the data of test participants but also that of their Facebook friends, exponentially expanding the dataset (Confessore, 2018).

  The collected information was used to construct psychological profiles and predictive models that were later sold to political campaigns to enable micro-targeted advertising during the 2016 U.S. presidential election. Facebook eventually discovered the misuse but failed to adequately inform users or prevent the data from being used for political purposes (Confessore, 2018).

  The major ethical dilemma at play here is: Can data, collected under one pretence, be used or repurposed for an entirely different purpose? In this case, it was used for political influence without informed consent. Here, information gathered under the guise of academic research was used for political manipulation without informed consent. The dilemma exposes deeper concerns about individual autonomy, privacy, and the integrity of democratic processes in an era dominated by large-scale data analytics.

 
 Refrence: [Article 1: The Guardian](https://www.theguardian.com/technology/2019/mar/17/the-cambridge-analytica-scandal-changed-the-world-but-it-didnt-change-facebook)
         | [Article 2: The New York Times](https://www.nytimes.com/2018/04/04/us/politics/cambridge-analytica-scandal-fallout.html)

  Authors: Julia Wong (Guardian), Nicholas Confessore (NYT)
format:
  html:
    css: style.css
    code-fold: true
about:
 template: jolla
 image: images/Zuck.jpeg 
 image-shape: rounded
 image-width: 25em
---

Considerations: 

1. What is the permission structure for using the data? Was it followed?

From Wong's Guardian Article, we learn that the permission structure claimed to be “academic research” for the users who consented to participate in the personality test. I compared the following information with my own research of Facebook's permissions structure. According to Wong, Facebook’s privacy policy did allow researchers to collect not only direct user data, but also that of friends (Wong 2018). Facebook does confirm this, in both court hearings and through legal documentation I was able to find. However, because friends did not explicitly give permission, this permission structure was compromised. Confessore builds on this, adding that Facebook’s own policy required that apps/tests ask for “a clear privacy policy and user authorization” and that Facebook has the ability to revoke data access when misuse is detected, but FB did not sufficiently alert impacted users (Confessore, 2018). Thus, although a formal permission structure existed (Facebook’s platform rules, app disclosure), it was not followed in practice by the parties involved.


2. What was the consent for recruiting participants?

According to the Wong through the Guardian, only a few thousand users who ended up installing the personality test app knowingly supplied their data under the pretense of “academic research”. None of their FB friends were asked directly for consent. From Confessore's article, we get the picture that Facebook’s terms required “informed consent” for data usage, yet the app’s true purpose (political targeting) was not disclosed (Confessore, 2018). Thus, the consent given was incomplete: it was partially informed and did not cover the large network of friends whose data was used.


3. Is the data being used in unintended ways?

Both articles make it clear that the data was being used in dishonest and manipulative ways. Wong details how Cambridge Analytica created psychographic models by transforming Facebook “likes” and friend networks into political persuasion mechanisms that could be weaponized by campaign strategists (Wong, 2018). Researchers inferred user traits using the OCEAN model (Openness, Conscientiousness, Extraversion, Agreeableness, and Neuroticism), which allowed them to predict how individuals might respond to targeted political messages. The company combined this with network analysis, exploiting friend connections and engagement metadata to build a comprehensive psychological map of the electorate. The New York Times emphasizes that Facebook’s algorithmic infrastructure enabled this misuse by allowing developers to collect not only users’ data but also that of their entire social networks, creating a massive dataset that could be repurposed for political manipulation (Confessore, 2018). This repurposing of data, gathered under the pretense of academic research, violated informed consent and exploited personal information for profit and political gain. The harm extended beyond individual privacy breaches; it undermined public trust, compromised user autonomy, and threatened the integrity of democratic processes. Facebook’s own rules required that data be used only for the purpose disclosed to users, yet in this case, that standard was clearly ignored. 


4. Who was being measured? 

The individuals actually measured in this case were the relatively small group of Facebook users, about 270,000 people who installed Aleksandr Kogan’s personality quiz app “This Is Your Digital Life.” “These users consented to share their personal data for what they believed was academic research” (Wong, 2018). However, as The New York Times reported, the app also collected information from the users’ Facebook friends, expanding the dataset to more than 50 million profiles that had never agreed to participate (Confessore, 2018). In other words, “the people whose data was taken were not the people who had given permission” (Wong, 2018). Cambridge Analytica then attempted to generalize the personality insights drawn from this small, non-representative sample to the broader Facebook population, and, by extension, to the U.S. electorate. This extrapolation was both scientifically unsound and ethically problematic because it applied inferences from a limited dataset to millions of individuals who were never directly measured or informed.


5. Summary: Why does this matter?

This case is uniquely pertinent, for it rests at the intersection of autonomy, democracy, and privacy. Those who benefited from the exploitation were data brokers, political strategists, and platform owners who exploited this breach for profit and influence (Wong, 2018). The individuals directly monitored were a small, consenting group of users who installed the quiz app. However, the harms transcended that sample. Millions of people who never consented to data collection had their personal information and "psychological traits inferred, modeled and used to shape and influence political behavior" (Confessore, 2018). As a result, both individual autonomy and democratic integrity were compromised through opaque micro-targeting and manipulation.

Since the scandal broke, Facebook and Cambridge Analytica have faced extensive legal and reputational fallout. Facebook claimed it removed the “This Is Your Digital Life” app in 2015 and required that all parties certify the deletion of improperly obtained data. In a statement following the exposure, Facebook vice president Paul Grewal emphasized that the company was “committed to vigorously enforcing our policies to protect people’s information” and would take any necessary steps to ensure compliance (Confessore, 2018). However, these actions came only after the misuse had already occurred, raising questions about the adequacy of Facebook’s oversight and accountability mechanisms.
Ultimately, the ethical violations in this case were driven more by profit and political gain than by transparency or public good. In a world where vast quantities of personal data can be harvested and repurposed in seconds, this episode serves as a cautionary example of the urgent need for stronger governance, informed consent standards, and ethical accountability in data-driven research and technology.
 





[image source: The Guardian](https://www.theguardian.com/technology/2019/mar/17/the-cambridge-analytica-scandal-changed-the-world-but-it-didnt-change-facebook)

Sources Cited: 

  [Wong, Julia Carrie. “The Cambridge Analytica Scandal Changed the World – but It Didn’t Change Facebook.” The Guardian, The Guardian, 18 Mar. 2019] (www.theguardian.com/technology/2019/mar/17/the-cambridge-analytica-scandal-changed-the-world-but-it-didnt-change-facebook)



  [Confessore, Nicholas. “Cambridge Analytica and Facebook: The Scandal and the Fallout so Far.” Nytimes.com, The New York Times, 4 Apr. 2018,] (www.nytimes.com/2018/04/04/us/politics/cambridge-analytica-scandal-fallout.html)

